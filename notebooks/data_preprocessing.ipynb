{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import scipy\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# ignore warnings jupyter notebook\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @todo: modify data to include all the dates from start to end, fill missing values with 0 do not remove any rows"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------- Functions ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to check if the data is complete\n",
    "def check_hours(df):\n",
    "    \"\"\"\"Function to double check if hours are complete, delete hours with more than 3 zeroes\"\"\"\n",
    "\n",
    "    minutes = ['00:00', '05:00', '10:00', '15:00', '20:00', '25:00', '30:00', '35:00', '40:00', '45:00', '50:00','55:00']\n",
    "    for date in df['timestamp'].dt.date.unique():\n",
    "        current_day = df[df['timestamp'].dt.date == date]\n",
    "        for hour in current_day['timestamp'].dt.hour.unique():\n",
    "            current_hour = current_day[current_day['timestamp'].dt.hour == hour]\n",
    "            if (len(current_hour) != 12): # 12 because we have 12, 5 minutes intervals\n",
    "                df.drop(current_hour.index, inplace=True)\n",
    "            # Q: should we delete hours with more than 3 zeroes? What would be the impact? A lot of zeroes in the cars column means that the data is sparse ?\n",
    "            # try:\n",
    "            #     if current_hour['cars'].value_counts()[0.0] > 3: # delete hours with more than 3 zeroes in the cars column \n",
    "            #         df.drop(current_hour.index, inplace=True)\n",
    "            # except Exception as e:\n",
    "            #     continue\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read folder with all csv files and create one df from it (one per intersection)\n",
    "def read_folder(current_intersection, configs, trac, direc):\n",
    "    \"\"\"Function to read all csv files (which is one per month) in the path and create one df from it.\"\"\"\n",
    "\n",
    "    print(\"Starting intersection: \" + str(current_intersection))  # note which intersection its working on\n",
    "    path = os.path.join(configs['data_folder'],current_intersection)  # define path to intersection folder (where all csv files are)\n",
    "    print(path)\n",
    "\n",
    "    df = pd.DataFrame(columns=['timestamp', 'cars'])  # create df to save everything in\n",
    "    # loop through all files that end with csv:\n",
    "    for file in os.listdir(path):  # read all files:\n",
    "        if file.endswith(\".csv\"):  # for all csv files in the folder\n",
    "\n",
    "            current_month = pd.read_csv(os.path.join(path,file), delimiter=\";\") # read csv file\n",
    "            cols = configs[\"trajectories\"][trac][direc][current_intersection] + [current_intersection]  # get sensors defined in config file + intersection name for dates (check csv files))\n",
    "            current_month = current_month[cols]  # only keep interesting columns\n",
    "            # some cleaning:\n",
    "            current_month = current_month[:-1]  # last row is totals\n",
    "            current_month = current_month.fillna(0)  # fill NA values with 0\n",
    "\n",
    "            # remove sensor errors:\n",
    "            # Q: why keep range 0 to 600?\n",
    "            # Q: why shift 4?\n",
    "            for sensor in configs['trajectories'][trac][direc][current_intersection]:\n",
    "                current_month[sensor] = current_month[sensor].apply(lambda x: x if x <= 600 else 0)  # remove sensor errors\n",
    "                current_month[sensor] = current_month[sensor].loc[current_month[sensor].shift(4) != current_month[sensor]] # remove sensor errors \n",
    "            \n",
    "            current_month[configs['trajectories'][trac][direc][current_intersection]] = \\\n",
    "                current_month[configs['trajectories'][trac][direc][current_intersection]].clip(-1,401)  # clip values between 0 and 400\n",
    "\n",
    "            # sum all sensors: \n",
    "            current_month['cars'] = current_month[configs['trajectories'][trac][direc][current_intersection]].sum(axis=1)  # sum of all interesting columns\n",
    "            current_month = current_month[[current_intersection, \"cars\"]]  # only keep name and total amount of cars\n",
    "            current_month.columns = ['timestamp', 'cars']  # rename to timestamp for general format\n",
    "            \n",
    "            # add to base df:\n",
    "            df = pd.concat([df, current_month])\n",
    "\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])  # format as dt\n",
    "    df['cars'] = df['cars'].clip(-1, len(configs['trajectories'][trac][direc][current_intersection] * 150))  # no intersection could be able to process sensors*150 cars\n",
    "    df = df.sort_values(by='timestamp')  #sort by timestamp\n",
    "    df = df.dropna()  # extra check to drop na values\n",
    "    df = df.loc[(df['timestamp'] > '2014-12-31') & (df['timestamp'] < '2020-05-31')]  # delete faulty datapoints outside scope\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = check_hours(df)  # try this afterwards\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to save the processed data for GNN training in h5 format\n",
    "def save_GNN_processed_data(raw_data,save_path):\n",
    "    # 1. from raw data dictionary create a dataframe with the values of the dictionary\n",
    "    # 2. from values in the dictionary change column name 'car' to key name \n",
    "    # 3. convert timestamp to datetime64\n",
    "    # 4. join df's on timestamp\n",
    "\n",
    "    first_intersection = list(raw_data.keys())[0]\n",
    "    base_df = raw_data[first_intersection]['timestamp']\n",
    "    base_df = pd.DataFrame(base_df)\n",
    "    # base_df['timestamp'] = np.datetime64(base_df['timestamp'])\n",
    "\n",
    "    for intersection in raw_data:\n",
    "        df = pd.DataFrame(raw_data[intersection])\n",
    "        df = df.rename(columns={'cars': intersection})\n",
    "        # df['timestamp'] = np.datetime64(df['timestamp'])\n",
    "        base_df = pd.merge(base_df, df, on='timestamp', how='inner')\n",
    "\n",
    "\n",
    "    base_df['timestamp'] = pd.to_datetime(base_df['timestamp'])\n",
    "    base_df['timestamp'] = np.array(base_df['timestamp'])\n",
    "\n",
    "\n",
    "    # set timestamp as index and remove the name of the index\n",
    "    base_df = base_df.set_index('timestamp')\n",
    "    base_df.index.name = None\n",
    "\n",
    "    # save the raw data to a h5 file\n",
    "    base_df.to_hdf(save_path, key='df', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to aggregate data into fpds per hour, a new probability column is added with the probability of a car passing through the intersection per 5 minutes\n",
    "def fpd(df, hours=1):\n",
    "    \"\"\"Function to aggregate a df of traffic info into one with fpds per window of x hours (this means 12*hours values)\"\"\"\n",
    "    freq = str(hours) + \"H\"\n",
    "    aggregate = df.groupby(pd.Grouper(freq=freq, key='timestamp')).sum()  # aggregate by 1 hour\n",
    "    df = pd.merge(df, aggregate, on='timestamp', how='left')  # merge with normal df\n",
    "    df = df.fillna(method='ffill')  # fill with previous number\n",
    "    df.columns = ['timestamp', 'cars', 'total']\n",
    "    df['cars'][df['cars'] < 0] = 0  # some inconistencies in the data where cars could be negative\n",
    "    df['total'][df['total'] <= 0] = 1  # some inconsistencies in the data where total cars could be negative, set to 1 to avoid problems\n",
    "    df['prob'] = df['cars'] / df['total'] # calculate probability\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weeks 7,\n",
    "# hours 24,\n",
    "# [ data, timesteps ]\n",
    "# data -> example: FPDs (list of 12 points) of all mondays 00:00 to 01:00 for 4 years with the list of probabilities for each FPD\n",
    "# timesteps -> example: date associated with each FPD of all mondays 00:00 to 01:00 for 4 years\n",
    "def create_timeslot_array(data,ohe_intersection,window=12):\n",
    "    \n",
    "    \"\"\"Function to reshape into numpy array shaped like (samples,window); e.g. 120 datapoints/12 (60min/5mins=12) = 10 FPDs.\n",
    "    This is neccesary to create the bhattacharyya matrices. Misfunctions when an hour in the data has more or fewer than 12 datapoints (happens with double timestamps or missing data)\n",
    "    Should be fixed by adding better data protection in the read_data function & rerunning the vlogbroker to output raw sensor values.\"\"\"\n",
    "\n",
    "    data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "    data['weekday'] = data['timestamp'].apply(lambda x: x.weekday())\n",
    "    data['hour'] = data['timestamp'].apply(lambda x: x.hour)\n",
    "    data_array = [] # create empty array\n",
    "    for i in range(7):\n",
    "        timeslots = []\n",
    "        for hour in range(24):\n",
    "            try:\n",
    "                datapoint = data[(data['weekday'] == i) & (data['hour'] == hour)]\n",
    "                x = np.array(datapoint['prob'])\n",
    "                x = x.reshape(int(len(x)/window), window)  # data should be complete and divisible by 12, otherwise it fails.\n",
    "                ohe_intersection_array = np.repeat(ohe_intersection, x.shape[0], axis=0)\n",
    "                hour_array = np.array([((hour+1)/24)]*x.shape[0]).reshape(-1,1)  # add in hour of day\n",
    "                week_array = np.array([((i+1)/7)]*x.shape[0]).reshape(-1,1)  # add in weekday\n",
    "                x = np.concatenate((x, hour_array, week_array,ohe_intersection_array), axis=1)  # add in normalized hour of day and weekday\n",
    "                dates = sorted(set(datapoint['timestamp'].apply(lambda x: x.floor(freq='H'))))  # add in hourly timestamp\n",
    "                timeslots.append([x, dates])\n",
    "            except Exception as e:\n",
    "                print(\"Exception in create_timeslot_array: \", i, hour)\n",
    "                print(e)\n",
    "        data_array.append(timeslots)\n",
    "    # output structure: data_array[7weekdays][24hours]; e.g. data_array[0][9] is data for monday mornings 9 am.\n",
    "    return data_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to save data to pickle file\n",
    "def save_pickle(data, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load data from pickle file\n",
    "def load_pickle(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -------- MAIN ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting intersection: K502\n",
      "../data/hauge/K502\n",
      "Starting intersection: K504\n",
      "../data/hauge/K504\n",
      "Starting intersection: K503\n",
      "../data/hauge/K503\n",
      "Starting intersection: K263\n",
      "../data/hauge/K263\n",
      "Starting intersection: K556\n",
      "../data/hauge/K556\n",
      "Starting intersection: K557\n",
      "../data/hauge/K557\n",
      "Starting intersection: K559\n",
      "../data/hauge/K559\n",
      "Starting intersection: K561\n",
      "../data/hauge/K561\n",
      "Starting intersection: K198\n",
      "../data/hauge/K198\n",
      "Starting intersection: K502\n",
      "../data/hauge/K502\n",
      "Starting intersection: K504\n",
      "../data/hauge/K504\n",
      "Starting intersection: K503\n",
      "../data/hauge/K503\n",
      "Starting intersection: K263\n",
      "../data/hauge/K263\n",
      "Starting intersection: K556\n",
      "../data/hauge/K556\n",
      "Starting intersection: K557\n",
      "../data/hauge/K557\n",
      "Starting intersection: K559\n",
      "../data/hauge/K559\n",
      "Starting intersection: K561\n",
      "../data/hauge/K561\n",
      "Starting intersection: K198\n",
      "../data/hauge/K198\n",
      "Starting intersection: K704\n",
      "../data/hauge/K704\n",
      "Starting intersection: K702\n",
      "../data/hauge/K702\n",
      "Starting intersection: K703\n",
      "../data/hauge/K703\n",
      "Starting intersection: K159\n",
      "../data/hauge/K159\n",
      "Starting intersection: K182\n",
      "../data/hauge/K182\n",
      "Starting intersection: K183\n",
      "../data/hauge/K183\n",
      "Starting intersection: K128\n",
      "../data/hauge/K128\n",
      "Starting intersection: K139\n",
      "../data/hauge/K139\n",
      "Starting intersection: K104\n",
      "../data/hauge/K104\n",
      "Starting intersection: K101\n",
      "../data/hauge/K101\n",
      "Starting intersection: K206\n",
      "../data/hauge/K206\n",
      "Starting intersection: K074\n",
      "../data/hauge/K074\n",
      "Starting intersection: K414\n",
      "../data/hauge/K414\n",
      "Starting intersection: K415\n",
      "../data/hauge/K415\n",
      "Starting intersection: K250\n",
      "../data/hauge/K250\n",
      "Starting intersection: K704\n",
      "../data/hauge/K704\n",
      "Starting intersection: K702\n",
      "../data/hauge/K702\n"
     ]
    }
   ],
   "source": [
    "# load configs\n",
    "with open(r\"../utils/configs.json\", 'r') as f:\n",
    "        configs = json.load(f)\n",
    "\n",
    "final_results = {}  # dictionary to store the results\n",
    "\n",
    "# loop over all trajectories and directions saperately \n",
    "for trajectory in configs['trajectories']:\n",
    "\n",
    "    # define one hot encoder for the intersections\n",
    "    enc_intersection = OneHotEncoder(handle_unknown='ignore')\n",
    "    intersections_list = list(configs['trajectories'][trajectory]['North'].keys())\n",
    "    enc_intersection.fit(np.array(intersections_list).reshape(-1,1))\n",
    "\n",
    "\n",
    "    final_results[trajectory] = {}\n",
    "    for direction in configs['trajectories'][trajectory].keys():\n",
    "        \n",
    "        # 1. first read the raw data from the pickle files and create a dictionary with the data:\n",
    "        raw_data = {}\n",
    "        for intersection in configs['trajectories'][trajectory][direction]:\n",
    "            raw_data[intersection] = read_folder(intersection, configs, trajectory, direction)\n",
    "            save_path = f\"../data/hauge/processed/GNN_raw_data_{direction}_{trajectory}.h5\" # path to save the processed raw data to a h5 file for GNNs\n",
    "            save_GNN_processed_data(raw_data, save_path) # save the raw data to a h5 file\n",
    "\n",
    "        # 2. create FPDs from dictionary \n",
    "        fpds = {}\n",
    "        fpd_hour = 1 # interval in hours to aggregate the data\n",
    "        for intersection in raw_data:\n",
    "            fpds[intersection] = fpd(raw_data[intersection], fpd_hour)\n",
    "\n",
    "        # 3. create timeslot arrays for further processing:\n",
    "        featured_fpds = {}  # array with shape (weeks, hours) containing the FPDs for each hour of each week for all days from 2018 to 2022\n",
    "        window_size = 12 # window size in 5 minute intervals \n",
    "        for intersection in fpds:\n",
    "            ohe_intersection = enc_intersection.transform(np.array([[intersection]])).toarray() # one hot encode the intersection\n",
    "            fpds_processed = create_timeslot_array(fpds[intersection], ohe_intersection, window_size) # create timeslot array\n",
    "            featured_fpds[intersection] = fpds_processed\n",
    "            \n",
    "        \n",
    "        # 4. save the featured fpds to a pickle file\n",
    "        featured_fpds_save_path = f\"../data/hauge/processed/featured_fpds_{direction}_{trajectory}.pickle\" # path to save the processed fpds to a pickle file\n",
    "        save_pickle(featured_fpds, featured_fpds_save_path) # save the fpds to a pickle file\n",
    "\n",
    "\n",
    "        # 5. save in final results dictionary\n",
    "        final_results[trajectory][direction] = featured_fpds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========== EXTRA ========="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ====== 1. Data Exploration ========"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K704 --> 2018-01-01 00:00:00 --- 2020-03-31 23:55:00\n",
      "K702 --> 2016-01-12 16:00:00 --- 2020-03-31 23:55:00\n",
      "K703 --> 2018-01-01 00:00:00 --- 2020-03-31 23:55:00\n",
      "K159 --> 2018-01-01 01:00:00 --- 2020-03-31 23:55:00\n",
      "K182 --> 2018-01-01 01:00:00 --- 2020-03-31 23:55:00\n",
      "K183 --> 2018-01-01 01:00:00 --- 2020-03-31 23:55:00\n",
      "K128 --> 2018-01-01 01:00:00 --- 2020-03-31 23:55:00\n",
      "K139 --> 2018-01-01 01:00:00 --- 2020-03-31 23:55:00\n",
      "K104 --> 2018-01-01 01:00:00 --- 2020-03-31 23:55:00\n",
      "K101 --> 2018-01-01 01:00:00 --- 2020-03-31 23:55:00\n",
      "K206 --> 2018-01-01 01:00:00 --- 2020-03-31 23:55:00\n",
      "K074 --> 2018-01-01 00:00:00 --- 2020-03-31 23:55:00\n",
      "K414 --> 2018-01-01 01:00:00 --- 2020-03-31 23:55:00\n",
      "K415 --> 2018-01-01 01:00:00 --- 2020-03-31 23:55:00\n",
      "K250 --> 2018-01-01 01:00:00 --- 2020-03-31 23:55:00\n"
     ]
    }
   ],
   "source": [
    "# all dates are correct\n",
    "for k in raw_data.keys():\n",
    "    print(k,'-->', raw_data[k]['timestamp'].iloc[0], \"---\" , raw_data[k]['timestamp'].iloc[-1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ======= 2. METER-LA and PEMS-BAY data processing ======="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>773869</th>\n",
       "      <th>767541</th>\n",
       "      <th>767542</th>\n",
       "      <th>717447</th>\n",
       "      <th>717446</th>\n",
       "      <th>717445</th>\n",
       "      <th>773062</th>\n",
       "      <th>767620</th>\n",
       "      <th>737529</th>\n",
       "      <th>717816</th>\n",
       "      <th>...</th>\n",
       "      <th>772167</th>\n",
       "      <th>769372</th>\n",
       "      <th>774204</th>\n",
       "      <th>769806</th>\n",
       "      <th>717590</th>\n",
       "      <th>717592</th>\n",
       "      <th>717595</th>\n",
       "      <th>772168</th>\n",
       "      <th>718141</th>\n",
       "      <th>769373</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-03-01 00:00:00</th>\n",
       "      <td>64.375000</td>\n",
       "      <td>67.625000</td>\n",
       "      <td>67.125000</td>\n",
       "      <td>61.500000</td>\n",
       "      <td>66.875000</td>\n",
       "      <td>68.750000</td>\n",
       "      <td>65.125</td>\n",
       "      <td>67.125</td>\n",
       "      <td>59.625000</td>\n",
       "      <td>62.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>45.625000</td>\n",
       "      <td>65.500</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>66.428571</td>\n",
       "      <td>66.875</td>\n",
       "      <td>59.375000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>59.250000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>61.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-03-01 00:05:00</th>\n",
       "      <td>62.666667</td>\n",
       "      <td>68.555556</td>\n",
       "      <td>65.444444</td>\n",
       "      <td>62.444444</td>\n",
       "      <td>64.444444</td>\n",
       "      <td>68.111111</td>\n",
       "      <td>65.000</td>\n",
       "      <td>65.000</td>\n",
       "      <td>57.444444</td>\n",
       "      <td>63.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>50.666667</td>\n",
       "      <td>69.875</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>58.555556</td>\n",
       "      <td>62.000</td>\n",
       "      <td>61.111111</td>\n",
       "      <td>64.444444</td>\n",
       "      <td>55.888889</td>\n",
       "      <td>68.444444</td>\n",
       "      <td>62.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-03-01 00:10:00</th>\n",
       "      <td>64.000000</td>\n",
       "      <td>63.750000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>66.500000</td>\n",
       "      <td>66.250000</td>\n",
       "      <td>64.500</td>\n",
       "      <td>64.250</td>\n",
       "      <td>63.875000</td>\n",
       "      <td>65.375000</td>\n",
       "      <td>...</td>\n",
       "      <td>44.125000</td>\n",
       "      <td>69.000</td>\n",
       "      <td>56.500000</td>\n",
       "      <td>59.250000</td>\n",
       "      <td>68.125</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>65.625000</td>\n",
       "      <td>61.375000</td>\n",
       "      <td>69.857143</td>\n",
       "      <td>62.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-03-01 00:15:00</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-03-01 00:20:00</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 207 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        773869     767541     767542     717447     717446  \\\n",
       "2012-03-01 00:00:00  64.375000  67.625000  67.125000  61.500000  66.875000   \n",
       "2012-03-01 00:05:00  62.666667  68.555556  65.444444  62.444444  64.444444   \n",
       "2012-03-01 00:10:00  64.000000  63.750000  60.000000  59.000000  66.500000   \n",
       "2012-03-01 00:15:00   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "2012-03-01 00:20:00   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "\n",
       "                        717445  773062  767620     737529     717816  ...  \\\n",
       "2012-03-01 00:00:00  68.750000  65.125  67.125  59.625000  62.750000  ...   \n",
       "2012-03-01 00:05:00  68.111111  65.000  65.000  57.444444  63.333333  ...   \n",
       "2012-03-01 00:10:00  66.250000  64.500  64.250  63.875000  65.375000  ...   \n",
       "2012-03-01 00:15:00   0.000000   0.000   0.000   0.000000   0.000000  ...   \n",
       "2012-03-01 00:20:00   0.000000   0.000   0.000   0.000000   0.000000  ...   \n",
       "\n",
       "                        772167  769372     774204     769806  717590  \\\n",
       "2012-03-01 00:00:00  45.625000  65.500  64.500000  66.428571  66.875   \n",
       "2012-03-01 00:05:00  50.666667  69.875  66.666667  58.555556  62.000   \n",
       "2012-03-01 00:10:00  44.125000  69.000  56.500000  59.250000  68.125   \n",
       "2012-03-01 00:15:00   0.000000   0.000   0.000000   0.000000   0.000   \n",
       "2012-03-01 00:20:00   0.000000   0.000   0.000000   0.000000   0.000   \n",
       "\n",
       "                        717592     717595     772168     718141  769373  \n",
       "2012-03-01 00:00:00  59.375000  69.000000  59.250000  69.000000  61.875  \n",
       "2012-03-01 00:05:00  61.111111  64.444444  55.888889  68.444444  62.875  \n",
       "2012-03-01 00:10:00  62.500000  65.625000  61.375000  69.857143  62.000  \n",
       "2012-03-01 00:15:00   0.000000   0.000000   0.000000   0.000000   0.000  \n",
       "2012-03-01 00:20:00   0.000000   0.000000   0.000000   0.000000   0.000  \n",
       "\n",
       "[5 rows x 207 columns]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the h5 file from meter-la data and convert to dictionary with each column name as key\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read the data from the h5 file and convert to dictionary with each column name as key\n",
    "def read_h5(df):\n",
    "    data_dict = {} # create empty dictionary for storing the data\n",
    "    # make datetime object using year, month, day, hour, minute, second\n",
    "    # reset index and add timestamp column\n",
    "    df.index = pd.to_datetime(df.index.year*10000000000 + df.index.month*100000000 + df.index.day*1000000 + df.index.hour*10000 + df.index.minute*100 + df.index.second, format='%Y%m%d%H%M%S')\n",
    "    df['timestamp'] = df.index\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    # create dictionary with each column name as key\n",
    "    for column in df.columns:\n",
    "        temp_df = pd.DataFrame()\n",
    "        temp_df['timestamp'] = df['timestamp']\n",
    "        temp_df['cars'] = df[column].values\n",
    "        data_dict[column] = temp_df\n",
    "\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>773869</th>\n",
       "      <th>767541</th>\n",
       "      <th>767542</th>\n",
       "      <th>717447</th>\n",
       "      <th>717446</th>\n",
       "      <th>717445</th>\n",
       "      <th>773062</th>\n",
       "      <th>767620</th>\n",
       "      <th>737529</th>\n",
       "      <th>717816</th>\n",
       "      <th>...</th>\n",
       "      <th>772167</th>\n",
       "      <th>769372</th>\n",
       "      <th>774204</th>\n",
       "      <th>769806</th>\n",
       "      <th>717590</th>\n",
       "      <th>717592</th>\n",
       "      <th>717595</th>\n",
       "      <th>772168</th>\n",
       "      <th>718141</th>\n",
       "      <th>769373</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-03-01 00:00:00</th>\n",
       "      <td>64.375000</td>\n",
       "      <td>67.625000</td>\n",
       "      <td>67.125000</td>\n",
       "      <td>61.500000</td>\n",
       "      <td>66.875000</td>\n",
       "      <td>68.750000</td>\n",
       "      <td>65.125</td>\n",
       "      <td>67.125</td>\n",
       "      <td>59.625000</td>\n",
       "      <td>62.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>45.625000</td>\n",
       "      <td>65.500</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>66.428571</td>\n",
       "      <td>66.875</td>\n",
       "      <td>59.375000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>59.250000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>61.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-03-01 00:05:00</th>\n",
       "      <td>62.666667</td>\n",
       "      <td>68.555556</td>\n",
       "      <td>65.444444</td>\n",
       "      <td>62.444444</td>\n",
       "      <td>64.444444</td>\n",
       "      <td>68.111111</td>\n",
       "      <td>65.000</td>\n",
       "      <td>65.000</td>\n",
       "      <td>57.444444</td>\n",
       "      <td>63.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>50.666667</td>\n",
       "      <td>69.875</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>58.555556</td>\n",
       "      <td>62.000</td>\n",
       "      <td>61.111111</td>\n",
       "      <td>64.444444</td>\n",
       "      <td>55.888889</td>\n",
       "      <td>68.444444</td>\n",
       "      <td>62.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-03-01 00:10:00</th>\n",
       "      <td>64.000000</td>\n",
       "      <td>63.750000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>66.500000</td>\n",
       "      <td>66.250000</td>\n",
       "      <td>64.500</td>\n",
       "      <td>64.250</td>\n",
       "      <td>63.875000</td>\n",
       "      <td>65.375000</td>\n",
       "      <td>...</td>\n",
       "      <td>44.125000</td>\n",
       "      <td>69.000</td>\n",
       "      <td>56.500000</td>\n",
       "      <td>59.250000</td>\n",
       "      <td>68.125</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>65.625000</td>\n",
       "      <td>61.375000</td>\n",
       "      <td>69.857143</td>\n",
       "      <td>62.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-03-01 00:15:00</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-03-01 00:20:00</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 207 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        773869     767541     767542     717447     717446  \\\n",
       "2012-03-01 00:00:00  64.375000  67.625000  67.125000  61.500000  66.875000   \n",
       "2012-03-01 00:05:00  62.666667  68.555556  65.444444  62.444444  64.444444   \n",
       "2012-03-01 00:10:00  64.000000  63.750000  60.000000  59.000000  66.500000   \n",
       "2012-03-01 00:15:00   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "2012-03-01 00:20:00   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "\n",
       "                        717445  773062  767620     737529     717816  ...  \\\n",
       "2012-03-01 00:00:00  68.750000  65.125  67.125  59.625000  62.750000  ...   \n",
       "2012-03-01 00:05:00  68.111111  65.000  65.000  57.444444  63.333333  ...   \n",
       "2012-03-01 00:10:00  66.250000  64.500  64.250  63.875000  65.375000  ...   \n",
       "2012-03-01 00:15:00   0.000000   0.000   0.000   0.000000   0.000000  ...   \n",
       "2012-03-01 00:20:00   0.000000   0.000   0.000   0.000000   0.000000  ...   \n",
       "\n",
       "                        772167  769372     774204     769806  717590  \\\n",
       "2012-03-01 00:00:00  45.625000  65.500  64.500000  66.428571  66.875   \n",
       "2012-03-01 00:05:00  50.666667  69.875  66.666667  58.555556  62.000   \n",
       "2012-03-01 00:10:00  44.125000  69.000  56.500000  59.250000  68.125   \n",
       "2012-03-01 00:15:00   0.000000   0.000   0.000000   0.000000   0.000   \n",
       "2012-03-01 00:20:00   0.000000   0.000   0.000000   0.000000   0.000   \n",
       "\n",
       "                        717592     717595     772168     718141  769373  \n",
       "2012-03-01 00:00:00  59.375000  69.000000  59.250000  69.000000  61.875  \n",
       "2012-03-01 00:05:00  61.111111  64.444444  55.888889  68.444444  62.875  \n",
       "2012-03-01 00:10:00  62.500000  65.625000  61.375000  69.857143  62.000  \n",
       "2012-03-01 00:15:00   0.000000   0.000000   0.000000   0.000000   0.000  \n",
       "2012-03-01 00:20:00   0.000000   0.000000   0.000000   0.000000   0.000  \n",
       "\n",
       "[5 rows x 207 columns]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data of meter-la \n",
    "load_path = '../data/METR-LA/metr-la.h5'\n",
    "save_path = '../data/METR-LA/processed/OWRI_df_format.pickle'\n",
    "df = pd.read_hdf(load_path, 'df')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to dictionary\n",
    "OWRI_df_format = read_h5(df)\n",
    "# save data to pickle file\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump(OWRI_df_format, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>sensor_id</th>\n",
       "      <th>400001</th>\n",
       "      <th>400017</th>\n",
       "      <th>400030</th>\n",
       "      <th>400040</th>\n",
       "      <th>400045</th>\n",
       "      <th>400052</th>\n",
       "      <th>400057</th>\n",
       "      <th>400059</th>\n",
       "      <th>400065</th>\n",
       "      <th>400069</th>\n",
       "      <th>...</th>\n",
       "      <th>409525</th>\n",
       "      <th>409526</th>\n",
       "      <th>409528</th>\n",
       "      <th>409529</th>\n",
       "      <th>413026</th>\n",
       "      <th>413845</th>\n",
       "      <th>413877</th>\n",
       "      <th>413878</th>\n",
       "      <th>414284</th>\n",
       "      <th>414694</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-01-01 00:00:00</th>\n",
       "      <td>71.4</td>\n",
       "      <td>67.8</td>\n",
       "      <td>70.5</td>\n",
       "      <td>67.4</td>\n",
       "      <td>68.8</td>\n",
       "      <td>66.6</td>\n",
       "      <td>66.8</td>\n",
       "      <td>68.0</td>\n",
       "      <td>66.8</td>\n",
       "      <td>69.0</td>\n",
       "      <td>...</td>\n",
       "      <td>68.8</td>\n",
       "      <td>67.9</td>\n",
       "      <td>68.8</td>\n",
       "      <td>68.0</td>\n",
       "      <td>69.2</td>\n",
       "      <td>68.9</td>\n",
       "      <td>70.4</td>\n",
       "      <td>68.8</td>\n",
       "      <td>71.1</td>\n",
       "      <td>68.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-01 00:05:00</th>\n",
       "      <td>71.6</td>\n",
       "      <td>67.5</td>\n",
       "      <td>70.6</td>\n",
       "      <td>67.5</td>\n",
       "      <td>68.7</td>\n",
       "      <td>66.6</td>\n",
       "      <td>66.8</td>\n",
       "      <td>67.8</td>\n",
       "      <td>66.5</td>\n",
       "      <td>68.2</td>\n",
       "      <td>...</td>\n",
       "      <td>68.4</td>\n",
       "      <td>67.3</td>\n",
       "      <td>68.4</td>\n",
       "      <td>67.6</td>\n",
       "      <td>70.4</td>\n",
       "      <td>68.8</td>\n",
       "      <td>70.1</td>\n",
       "      <td>68.4</td>\n",
       "      <td>70.8</td>\n",
       "      <td>67.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-01 00:10:00</th>\n",
       "      <td>71.6</td>\n",
       "      <td>67.6</td>\n",
       "      <td>70.2</td>\n",
       "      <td>67.4</td>\n",
       "      <td>68.7</td>\n",
       "      <td>66.1</td>\n",
       "      <td>66.8</td>\n",
       "      <td>67.8</td>\n",
       "      <td>66.2</td>\n",
       "      <td>67.8</td>\n",
       "      <td>...</td>\n",
       "      <td>68.4</td>\n",
       "      <td>67.4</td>\n",
       "      <td>68.4</td>\n",
       "      <td>67.5</td>\n",
       "      <td>70.2</td>\n",
       "      <td>68.3</td>\n",
       "      <td>69.8</td>\n",
       "      <td>68.4</td>\n",
       "      <td>70.5</td>\n",
       "      <td>67.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-01 00:15:00</th>\n",
       "      <td>71.1</td>\n",
       "      <td>67.5</td>\n",
       "      <td>70.3</td>\n",
       "      <td>68.0</td>\n",
       "      <td>68.5</td>\n",
       "      <td>66.7</td>\n",
       "      <td>66.6</td>\n",
       "      <td>67.7</td>\n",
       "      <td>65.9</td>\n",
       "      <td>67.8</td>\n",
       "      <td>...</td>\n",
       "      <td>68.5</td>\n",
       "      <td>67.5</td>\n",
       "      <td>68.5</td>\n",
       "      <td>67.5</td>\n",
       "      <td>70.4</td>\n",
       "      <td>68.7</td>\n",
       "      <td>70.2</td>\n",
       "      <td>68.4</td>\n",
       "      <td>70.8</td>\n",
       "      <td>67.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-01 00:20:00</th>\n",
       "      <td>71.7</td>\n",
       "      <td>67.8</td>\n",
       "      <td>70.2</td>\n",
       "      <td>68.1</td>\n",
       "      <td>68.4</td>\n",
       "      <td>66.9</td>\n",
       "      <td>66.1</td>\n",
       "      <td>67.7</td>\n",
       "      <td>66.1</td>\n",
       "      <td>67.8</td>\n",
       "      <td>...</td>\n",
       "      <td>68.5</td>\n",
       "      <td>67.7</td>\n",
       "      <td>68.5</td>\n",
       "      <td>67.4</td>\n",
       "      <td>69.6</td>\n",
       "      <td>69.1</td>\n",
       "      <td>70.0</td>\n",
       "      <td>68.4</td>\n",
       "      <td>71.0</td>\n",
       "      <td>67.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 325 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "sensor_id            400001  400017  400030  400040  400045  400052  400057  \\\n",
       "2017-01-01 00:00:00    71.4    67.8    70.5    67.4    68.8    66.6    66.8   \n",
       "2017-01-01 00:05:00    71.6    67.5    70.6    67.5    68.7    66.6    66.8   \n",
       "2017-01-01 00:10:00    71.6    67.6    70.2    67.4    68.7    66.1    66.8   \n",
       "2017-01-01 00:15:00    71.1    67.5    70.3    68.0    68.5    66.7    66.6   \n",
       "2017-01-01 00:20:00    71.7    67.8    70.2    68.1    68.4    66.9    66.1   \n",
       "\n",
       "sensor_id            400059  400065  400069  ...  409525  409526  409528  \\\n",
       "2017-01-01 00:00:00    68.0    66.8    69.0  ...    68.8    67.9    68.8   \n",
       "2017-01-01 00:05:00    67.8    66.5    68.2  ...    68.4    67.3    68.4   \n",
       "2017-01-01 00:10:00    67.8    66.2    67.8  ...    68.4    67.4    68.4   \n",
       "2017-01-01 00:15:00    67.7    65.9    67.8  ...    68.5    67.5    68.5   \n",
       "2017-01-01 00:20:00    67.7    66.1    67.8  ...    68.5    67.7    68.5   \n",
       "\n",
       "sensor_id            409529  413026  413845  413877  413878  414284  414694  \n",
       "2017-01-01 00:00:00    68.0    69.2    68.9    70.4    68.8    71.1    68.0  \n",
       "2017-01-01 00:05:00    67.6    70.4    68.8    70.1    68.4    70.8    67.4  \n",
       "2017-01-01 00:10:00    67.5    70.2    68.3    69.8    68.4    70.5    67.9  \n",
       "2017-01-01 00:15:00    67.5    70.4    68.7    70.2    68.4    70.8    67.6  \n",
       "2017-01-01 00:20:00    67.4    69.6    69.1    70.0    68.4    71.0    67.9  \n",
       "\n",
       "[5 rows x 325 columns]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data of pems-bay\n",
    "load_path = '../data/PEMS-BAY/pems-bay.h5'\n",
    "save_path = '../data/PEMS-BAY/processed/OWRI_df_format.pickle'\n",
    "df = pd.read_hdf(load_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to dictionary\n",
    "OWRI_df_format = read_h5(df)\n",
    "# save data to pickle file\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump(OWRI_df_format, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ====== 3.Combining hauge all trajectory data ========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read hauge processed data\n",
    "load_path1 = '../data/hauge/processed/GNN_raw_data_North_T1.h5'\n",
    "df1 = pd.read_hdf(load_path1)\n",
    "# add \"_N\" to the column names of the North trajectories\n",
    "df1.columns = [str(col) + '_N' for col in df1.columns]\n",
    "\n",
    "load_path2 = '../data/hauge/processed/GNN_raw_data_North_T2.h5'\n",
    "df2 = pd.read_hdf(load_path2)\n",
    "# add \"_N\" to the column names of the North trajectories\n",
    "df2.columns = [str(col) + '_N' for col in df2.columns]\n",
    "\n",
    "load_path3 = '../data/hauge/processed/GNN_raw_data_South_T1.h5'\n",
    "df3 = pd.read_hdf(load_path3)\n",
    "# add \"_S\" to the column names of the South trajectories\n",
    "df3.columns = [str(col) + '_S' for col in df3.columns]\n",
    "\n",
    "load_path4 = '../data/hauge/processed/GNN_raw_data_South_T2.h5'\n",
    "df4 = pd.read_hdf(load_path4)\n",
    "# add \"_S\" to the column names of the South trajectories\n",
    "df4.columns = [str(col) + '_S' for col in df4.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the dataframes on index\n",
    "df = pd.concat([df1, df2, df3, df4], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55788, 48)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "K502_N    0.0\n",
       "K504_N    0.0\n",
       "K503_N    0.0\n",
       "K263_N    0.0\n",
       "K556_N    0.0\n",
       "K557_N    0.0\n",
       "K559_N    0.0\n",
       "K561_N    0.0\n",
       "K198_N    0.0\n",
       "K704_N    0.0\n",
       "K702_N    0.0\n",
       "K703_N    0.0\n",
       "K159_N    0.0\n",
       "K182_N    0.0\n",
       "K183_N    0.0\n",
       "K128_N    0.0\n",
       "K139_N    0.0\n",
       "K104_N    0.0\n",
       "K101_N    0.0\n",
       "K206_N    0.0\n",
       "K074_N    0.0\n",
       "K414_N    0.0\n",
       "K415_N    0.0\n",
       "K250_N    0.0\n",
       "K502_S    0.0\n",
       "K504_S    0.0\n",
       "K503_S    0.0\n",
       "K263_S    0.0\n",
       "K556_S    0.0\n",
       "K557_S    0.0\n",
       "K559_S    0.0\n",
       "K561_S    0.0\n",
       "K198_S    0.0\n",
       "K704_S    0.0\n",
       "K702_S    0.0\n",
       "K703_S    0.0\n",
       "K159_S    0.0\n",
       "K182_S    0.0\n",
       "K183_S    0.0\n",
       "K128_S    0.0\n",
       "K139_S    0.0\n",
       "K104_S    0.0\n",
       "K101_S    0.0\n",
       "K206_S    0.0\n",
       "K074_S    0.0\n",
       "K414_S    0.0\n",
       "K415_S    0.0\n",
       "K250_S    0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check null percentage of each column\n",
    "df.isnull().sum()/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>K074_N</th>\n",
       "      <th>K074_S</th>\n",
       "      <th>K101_N</th>\n",
       "      <th>K101_S</th>\n",
       "      <th>K104_N</th>\n",
       "      <th>K104_S</th>\n",
       "      <th>K128_N</th>\n",
       "      <th>K128_S</th>\n",
       "      <th>K139_N</th>\n",
       "      <th>K139_S</th>\n",
       "      <th>...</th>\n",
       "      <th>K559_N</th>\n",
       "      <th>K559_S</th>\n",
       "      <th>K561_N</th>\n",
       "      <th>K561_S</th>\n",
       "      <th>K702_N</th>\n",
       "      <th>K702_S</th>\n",
       "      <th>K703_N</th>\n",
       "      <th>K703_S</th>\n",
       "      <th>K704_N</th>\n",
       "      <th>K704_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-01 01:00:00</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>...</td>\n",
       "      <td>28.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 01:05:00</th>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>22.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 01:10:00</th>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>...</td>\n",
       "      <td>24.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 01:15:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 01:20:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>...</td>\n",
       "      <td>47.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-31 23:35:00</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>21.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-31 23:40:00</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-31 23:45:00</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-31 23:50:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-31 23:55:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55788 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     K074_N  K074_S  K101_N  K101_S  K104_N  K104_S  K128_N  \\\n",
       "2018-01-01 01:00:00     6.0     0.0    59.0    85.0    56.0    52.0    31.0   \n",
       "2018-01-01 01:05:00     4.0     9.0    29.0    60.0    57.0    58.0    29.0   \n",
       "2018-01-01 01:10:00     6.0    12.0    33.0    69.0    48.0    63.0    25.0   \n",
       "2018-01-01 01:15:00     0.0    15.0    55.0    72.0    51.0    49.0    20.0   \n",
       "2018-01-01 01:20:00     0.0    10.0    57.0    75.0     9.0    63.0    34.0   \n",
       "...                     ...     ...     ...     ...     ...     ...     ...   \n",
       "2020-03-31 23:35:00     3.0     3.0    21.0    12.0    14.0    20.0    10.0   \n",
       "2020-03-31 23:40:00     3.0     2.0    13.0    13.0    14.0    22.0    12.0   \n",
       "2020-03-31 23:45:00     3.0     2.0    12.0    23.0    17.0    14.0     6.0   \n",
       "2020-03-31 23:50:00     0.0     3.0    14.0     9.0    27.0     9.0     9.0   \n",
       "2020-03-31 23:55:00     0.0     1.0    10.0    10.0    17.0    12.0     8.0   \n",
       "\n",
       "                     K128_S  K139_N  K139_S  ...  K559_N  K559_S  K561_N  \\\n",
       "2018-01-01 01:00:00    57.0    30.0    51.0  ...    28.0    41.0    24.0   \n",
       "2018-01-01 01:05:00    56.0    31.0    22.0  ...    22.0    44.0    21.0   \n",
       "2018-01-01 01:10:00    50.0    22.0    51.0  ...    24.0    28.0    17.0   \n",
       "2018-01-01 01:15:00    52.0    13.0    50.0  ...     9.0    57.0    31.0   \n",
       "2018-01-01 01:20:00    46.0    34.0    45.0  ...    47.0    35.0    43.0   \n",
       "...                     ...     ...     ...  ...     ...     ...     ...   \n",
       "2020-03-31 23:35:00    12.0    19.0    10.0  ...    21.0    11.0    20.0   \n",
       "2020-03-31 23:40:00     8.0     5.0    12.0  ...     9.0    10.0    11.0   \n",
       "2020-03-31 23:45:00     5.0     8.0     8.0  ...    20.0     9.0    16.0   \n",
       "2020-03-31 23:50:00     2.0     3.0     9.0  ...    13.0     7.0    13.0   \n",
       "2020-03-31 23:55:00    10.0     7.0     5.0  ...     8.0    11.0     7.0   \n",
       "\n",
       "                     K561_S  K702_N  K702_S  K703_N  K703_S  K704_N  K704_S  \n",
       "2018-01-01 01:00:00    50.0    24.0    21.0    54.0    49.0    28.0    41.0  \n",
       "2018-01-01 01:05:00    53.0    42.0    35.0    55.0    59.0    24.0    41.0  \n",
       "2018-01-01 01:10:00    36.0    41.0    34.0    52.0    26.0    40.0    40.0  \n",
       "2018-01-01 01:15:00    56.0    43.0    32.0    53.0    57.0    46.0    33.0  \n",
       "2018-01-01 01:20:00    63.0    46.0    39.0    60.0    68.0     6.0    44.0  \n",
       "...                     ...     ...     ...     ...     ...     ...     ...  \n",
       "2020-03-31 23:35:00    17.0     4.0     6.0     9.0     5.0     4.0     7.0  \n",
       "2020-03-31 23:40:00     2.0     5.0     2.0     5.0     6.0     1.0     8.0  \n",
       "2020-03-31 23:45:00    15.0     7.0     6.0     4.0    11.0     8.0     2.0  \n",
       "2020-03-31 23:50:00    10.0     1.0     7.0     2.0     4.0     5.0     1.0  \n",
       "2020-03-31 23:55:00    12.0     5.0     2.0     5.0     5.0     0.0     3.0  \n",
       "\n",
       "[55788 rows x 48 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort the columns\n",
    "df = df.reindex(sorted(df.columns), axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the raw data to a h5 file\n",
    "save_path = '../data/hauge/processed/GNN_raw_data.h5'\n",
    "df.to_hdf(save_path, key='df', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data correction in Trejectory 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K159_path = '../data/hauge/K159/'\n",
    "# correction_list = ['K159-2018-1-.csv','K159-2018-2-.csv','K159-2018-3-.csv','K159-2018-4-.csv','K159-2018-5-.csv','K159-2018-6-.csv','K159-2018-7-1.csv','K159-2018-7-2.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loop over all files in the folder and correct the column names\n",
    "# for ls in os.listdir(K159_path):\n",
    "#     if ls in correction_list:\n",
    "#         print(ls)\n",
    "#         load_path = os.path.join(K159_path, ls)\n",
    "#         df = pd.read_csv(load_path, sep=';')\n",
    "#         df.rename(columns={'21': '021', '81': '081','51': '051', '711':'713'}, inplace=True)\n",
    "#         df.to_csv(load_path, sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # merge the two files for 2018-7 and save to a new file\n",
    "# csv1 = pd.read_csv('../data/hauge/K159/K159-2018-7-1.csv', sep=';')\n",
    "# csv2 = pd.read_csv('../data/hauge/K159/K159-2018-7-2.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comb_csv = pd.concat([csv1, csv2]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comb_csv.to_csv('../data/hauge/K159/K159-2018-7-.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OWRI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
